# Knowledge

## AI Cheatsheet

<!-- image -->

## AI Maturity Model

The AI Maturity Model is a framework used to assess an organization's level of AI adoption and integration across different stages. It helps stakeholders understand where their organization stands and what steps are needed to advance AI capabilities.

- 1. Awareness: The organization recognizes AI as a key area of potential but lacks a clear strategy or significant investment.
- Key Characteristics:
- Awareness of AI technologies and trends.
- High-level discussions on AI with limited understanding or action.
- Minimal experimentation or pilot projects.
- 3. Operational: AI has moved into production with operational applications that follow best practices for implementation and scaling.
- Key Characteristics:
- AI models are deployed in production, providing valuable insights or automating processes.
- Establishment of best practices for model governance,
- monitoring, and continuous improvement.
- Integration of AI into existing business processes but may still be siloed in specific functions.

<!-- image -->

5. Transformational: AI is embedded into the organization's DNA, driving transformation across all levels of the business, from operations to culture.

- Key Characteristics:
- AI is a driving force for business model innovation and differentiation.
- Continuous improvement through AI becomes second nature to the organization, deeply embedded in the culture.
- Data-driven decision-making and automation are pervasive across all business functions, creating a competitive advantage.

## Key Takeaways for Executives:

- Awareness: Invest in education and awareness to build a foundational understanding of AI.
- Active: Focus on experimentation and pilot projects, setting up the groundwork for practical AI applications.
- Operational: Scale AI applications into production and establish robust operational best practices.
- Systemic: Ensure AI is an integrated part of the business strategy and daily operations.
- Transformational: Foster a culture where AI is central to continuous innovation and business transformation.
- . Active (Experimentation): The organization is actively experimenting with AI, typically in a data science context, through small pilot projects.
- Key Characteristics:
- Initial AI projects in research and development, often within isolated business units or departments.
- Focus on experimentation and proof of concepts, mostly by data science teams.
- Early-stage tools and models, possibly involving predictive analytics or natural language processing.
- 4. Systemic: AI is fully integrated into the organization's digital strategy and core business processes, with a structured AI strategy and governance in place.
- Key Characteristics:
- AI is an integral part of many projects and decision-making processes.
- Clear AI strategy in place, with structured governance, model deployment pipelines, and cross-functional collaboration.
- AI's value is recognized across departments, driving innovation and operational efficiency.

## AI Cheatsheet

| Term                    | Definition                                                                                | Notes   |
|-------------------------|-------------------------------------------------------------------------------------------|---------|
| Assistants              | AI systems designed to help users with tasks via natural language interfaces              |         |
| Agentic AI              | AI that acts autonomously to achieve goals, often involving decision-making and planning. |         |
| Bag of Words            | Atext representation method treating text as an unordered collection of words.            |         |
| Benchmarking            | Evaluating models against standard datasets to measure performance and compare systems.   |         |
| Chain-of-thought        | Areasoning approach where intermediate steps are generated to aid in solving tasks.       |         |
| Classification          | Assigning input data to predefined categories or labels.                                  |         |
| Clustering              | Grouping similar data points together without labeled outcomes (unsupervised).            |         |
| Collaborative Filtering | Arecommendation method based on user-item interaction patterns.                           |         |
| CommonCrawl             | Apublicly available web dataset used for large-scale language model training.             |         |
| Context Window          | The amount of text a language model can consider at once when generating output.          |         |
| Data Augmentation       | Techniques to increase dataset size by modifying existing data samples.                   |         |
| Deep Learning           | Asubset of machine learning using neural networks with many layers.                       |         |
| Edge AI                 | Running AI models locally on edge devices for low-latency and privacy.                    |         |
| Embeddings              | Vector representations of data (e.g., words, images) capturing semantic meaning.          |         |
| Explainability          | The ability to understand and interpret how and why a model makes decisions.              |         |
| Explainable AI (XAI)    | AI systems designed to provide transparent, interpretable decisions.                      |         |
| Feature                 | Anindividual measurable property or characteristic used in model training.                |         |
| Federated Learning      | Training models across decentralized data sources without data sharing.                   |         |
| Fine-Tuning             | Adapting a pre-trained model to a specific task with new data.                            |         |

## AI Cheatsheet

| Term                              | Definition                                                                                                                             | Notes   |
|-----------------------------------|----------------------------------------------------------------------------------------------------------------------------------------|---------|
| Genrative AI (GenAI)              | AI that creates new content such as text, images, or audio.                                                                            |         |
| Generative Model                  | Amodel that learns to generate new data similar to its training set.                                                                   |         |
| Generative Pre-Trained (GPT)      | Generative Pre-trained Transformer, a language model developed by OpenAI.                                                              |         |
| Hallucination                     | When a model generates plausible but incorrect or nonsensical information.                                                             |         |
| Image Classification              | Assigning labels to images based on their visual content.                                                                              |         |
| Inference                         | The process of making predictions using a trained model.                                                                               |         |
| Label                             | The target output assigned to a data point in supervised learning.                                                                     |         |
| Learning Rate                     | Ahyperparameter controlling the step size during model training.                                                                       |         |
| LLM                               | Large Language Model trained on massive text datasets for natural language tasks.                                                      |         |
| Model                             | Amathematical representation trained to make predictions or decisions from data.                                                       |         |
| Model Fine-Tuning                 | See Fine-tuning                                                                                                                        |         |
| Model Weights                     | Parameters learned by a model that determine its behavior.                                                                             |         |
| Multimodal AI                     | AI that processes and integrates multiple data types (e.g., text and images).                                                          |         |
| Natural Language Processing (NLP) | AI focused on interpreting, generating, and interacting with human language-frequently central to product features and search systems. |         |
| OpenAI                            | AnAIresearch lab and company behind GPTand DALL-E-often considered in vendor decisions and model sourcing.                             |         |
| Parameters                        | The internal values a model learns during training-key when evaluating model complexity and infrastructure needs.                      |         |
| Privacy-Preserving AI             | Techniques ensuring AI models don't compromise sensitive data-important for regulated industries and user trust.                       |         |
| Prompt                            | The input text given to a language model-crucial in determining output quality and user experience in GenAI features.                  |         |
| Prompt Engineering                | The practice of designing effective prompts to guide LLMbehavior-often an iterative process to align AI output with product needs.     |         |

## AI Cheatsheet

| Term                     | Definition                                                                                                                          | Notes   |
|--------------------------|-------------------------------------------------------------------------------------------------------------------------------------|---------|
| Recommender System       | AI systems that suggest content to users based on data-vital in ecommerce, media, and personalization efforts.                      |         |
| RLHF                     | Reinforcement Learning from HumanFeedback, used to align LLMs with human values-important in safe, aligned model behavior.          |         |
| Robotics                 | Combines AI with mechanical systems to perform tasks-relevant in automation, logistics, and manufacturing initiatives.              |         |
| ROCCurve                 | Avisualization of model performance across thresholds-used in evaluating tradeoffs between false positives and negatives.           |         |
| Self-supervised Learning | Amethod where models learn from unlabeled data-valuable for scaling training when labels are scarce.                                |         |
| Singularity              | Ahypothetical point where AI surpasses human intelligence-typically referenced in strategic, long-term discussions.                 |         |
| Softmax Function         | Amathematical function used in classification tasks to turn outputs into probabilities-important when debugging or refining models. |         |
| Speech-to-Text (STT)     | AI that converts spoken language into written text-relevant for accessibility and transcription features.                           |         |
| Supervised Learning      | Learning from labeled data-most commonly used in real-world ML applications like classification and regression.                     |         |
| Synthetic Data           | Artificially generated data used for training models-useful when real data is limited or sensitive.                                 |         |
| Temperature              | Atuning parameter that controls randomness in model output-often adjusted during prompt experiments.                                |         |
| Text-to-Speech (TTS)     | AI that converts written text into spoken audio-used in accessibility, voice assistants, and media apps.                            |         |
| TF-IDF                   | Alegacy technique for text analysis that weighs word importance-helpful in search relevance or feature engineering discussions.     |         |
| The Pile                 | Anopen-source dataset used to train LLMs-useful when evaluating model lineage or biases.                                            |         |
| Token                    | Achunk of text (word or subword) used in model processing-important for understanding context limits and cost.                      |         |
| Tokenization             | The process of splitting text into tokens for model input-often affects model performance and context handling.                     |         |

## AI Cheatsheet

| Term                  | Definition                                                                                                        | Notes   |
|-----------------------|-------------------------------------------------------------------------------------------------------------------|---------|
| Tokenization          | The process of splitting text into tokens for model input-often affects model performance and context handling.   |         |
| Training              | The phase where a model learns from data-typically resource-intensive and central to development cycles.          |         |
| Training Loss         | Ametric showing how far the model's predictions are from expected results- monitored during model training.       |         |
| Transfer Learning     | Reusing parts of a trained model for a new task-saves compute and accelerates model deployment.                   |         |
| Transformers          | The core architecture behind most LLMs-important when evaluating model performance and scaling strategies.        |         |
| Transparency          | Clarity in how models work and make decisions-important for governance, compliance, and stakeholder trust.        |         |
| TTS                   | See Text-to-Speech.                                                                                               |         |
| Turing Test           | Abenchmark for evaluating if an AI can mimic human behavior-referenced in conversations about AGI readiness.      |         |
| Underfitting          | When a model fails to learn patterns from the training data-relevant when diagnosing poor performance.            |         |
| Unsupervised Learning | Learning from data without labels-used for discovery, clustering, and dimensionality reduction.                   |         |
| Voice Recognition     | AI that identifies who is speaking-important in security, personalization, and telephony features.                |         |
| Whisper               | OpenAI's model for speech recognition-often used in transcription and multilingual voice-based applications.      |         |
| Word Embedding        | Vector representations of words that capture meaning-used in NLPto improve understanding and similarity measures. |         |
| Word2Vec              | Aclassic word embedding technique-relevant in early NLPsystems and benchmarking against newer models.             |         |

## AI Cheatsheet

| Term                               | Definition                                                                                                                     | Notes   |
|------------------------------------|--------------------------------------------------------------------------------------------------------------------------------|---------|
| Activation Function                | Amathematical function applied to neurons in a neural network-crucial in determining model output behavior and complexity.     |         |
| Artificial Neural Network (ANN)    | Acomputational model inspired by the brain's structure-foundational to most modern deep learning systems.                      |         |
| Attention Mechanism                | Atechnique that allows models to focus on relevant parts of input data-key to the success of transformer-based models.         |         |
| AUC(Area Under the Curve)          | Aperformance metric used to evaluate classification models-helps in selecting models based on precision-recall tradeoffs.      |         |
| Autoencoder                        | Aneural network used for dimensionality reduction or anomaly detection- often useful in pretraining and compression pipelines. |         |
| Autoregression                     | Amodel that predicts future values based on past ones-common in time series forecasting and language generation.               |         |
| Backpropagation                    | The core algorithm used to train neural networks by minimizing loss-essential for understanding model convergence.             |         |
| Batch Size                         | The number of training samples processed at once-affects model accuracy, memory usage, and training speed.                     |         |
| Bias                               | Aconsistent error in model predictions-important when evaluating fairness and generalization.                                  |         |
| Bias Mitigation                    | Techniques used to reduce unwanted biases in AI systems-especially critical in regulated industries.                           |         |
| Bias-Variance Tradeoff             | The balance between underfitting and overfitting-important for model tuning and cross-validation discussions.                  |         |
| Confusion Matrix                   | Atable used to describe model performance for classification-helps pinpoint false positives and negatives.                     |         |
| Content-Based Filtering            | Arecommendation method based on item features-used when user behavior data is sparse.                                          |         |
| Convolutional Neural Network (CNN) | Adeep learning architecture primarily used for image and video processing- frequent in vision-based AI products.               |         |

## AI Cheatsheet

| Term                                           | Definition                                                                                                            | Notes   |
|------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------|---------|
| Cross-validation                               | Atechnique for validating models using multiple data splits-helps ensure robustness and avoid overfitting.            |         |
| Decoder-only                                   | Atransformer architecture used in models like GPT-optimized for generating text rather than understanding it.         |         |
| Discriminative Model                           | Amodel that learns the boundary between classes-used in classification tasks where prediction accuracy is key.        |         |
| Dropout                                        | Aregularization technique that disables random neurons during training- improves generalization.                      |         |
| Early Stopping                                 | Atechnique to halt training when performance stops improving-helps prevent overfitting.                               |         |
| Encoder-decoder                                | Atransformer architecture for tasks like translation-used when both input and output sequences matter.                |         |
| Encoder-only                                   | Used in models like BERT-optimized for understanding and analyzing input sequences.                                   |         |
| Epoch                                          | One full pass through the training dataset-used in planning and monitoring model training duration.                   |         |
| F1 Score                                       | Ametric combining precision and recall-especially useful when data is imbalanced.                                     |         |
| Feature Engineering                            | The process of selecting and transforming variables to improve model performance-often where domain knowledge shines. |         |
| GloVe (Global Vectors for Word Representation) | Anearly word embedding technique-commonly used in NLPbenchmarks.                                                      |         |
| Gradient Clipping                              | Amethod to prevent exploding gradients during training-useful in RNNsand deep models.                                 |         |
| Gradient Descent                               | Anoptimization algorithm for minimizing loss functions-core to most model training pipelines.                         |         |
| Hyperparameters                                | Model configuration settings set before training-impact accuracy, efficiency, and training time.                      |         |
| Image Segmentation                             | Dividing an image into meaningful parts-used in medical imaging, autonomous vehicles, and more.                       |         |

## AI Cheatsheet

| Term              | Definition                                                                                                   | Notes   |
|-------------------|--------------------------------------------------------------------------------------------------------------|---------|
| Interpretability  | Understanding a model's internal logic-important for trust and debugging in AI systems.                      |         |
| L1 Regularization | Amethod that encourages sparsity in models-used to reduce complexity and prevent overfitting.                |         |
| L2 Regularization | Amethod that discourages large model weights-helps generalize and stabilize training.                        |         |
| Latent Space      | Anabstract representation of data learned by models-useful when visualizing embeddings or model compression. |         |
| Logits            | Raw model outputs before probabilities-used in loss calculations and debugging classification issues.        |         |
